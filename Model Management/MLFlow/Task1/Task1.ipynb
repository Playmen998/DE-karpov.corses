{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39d994b-fcb7-4966-9d5f-fa9e51fc80ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994d0c34-9c05-4973-96e6-8fd5a4218053",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = 'has_car_accident'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13b9e83-f7dc-49e3-9dd3-1e2e055f10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(train_alg):\n",
    "    \"\"\"\n",
    "    Создание пайплаина над выбранной моделью.\n",
    "\n",
    "    :return: Pipeline\n",
    "    \"\"\"\n",
    "    sex_indexer = StringIndexer(inputCol='sex',\n",
    "                                outputCol=\"sex_index\")\n",
    "    car_class_indexer = StringIndexer(inputCol='car_class',\n",
    "                                      outputCol=\"car_class_index\")\n",
    "    features = [\"age\", \"sex_index\", \"car_class_index\", \"driving_experience\",\n",
    "                \"speeding_penalties\", \"parking_penalties\", \"total_car_accident\"]\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "    return Pipeline(stages=[sex_indexer, car_class_indexer, assembler, train_alg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9480e4-4585-439c-811c-69f9f64c133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(evaluator, predict, metric_list):\n",
    "    for metric in metric_list:\n",
    "        evaluator.setMetricName(metric)\n",
    "        score = evaluator.evaluate(predict)\n",
    "        mlflow.log_metric(f\"{metric}\", score)\n",
    "        print(f\"{metric} score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9677e9-eeba-42f4-a87a-98434635e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(pipeline, gbt, train_df, evaluator):\n",
    "    grid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "        .addGrid(gbt.maxIter, [20, 30]) \\\n",
    "        .addGrid(gbt.maxBins, [16, 32]) \\\n",
    "        .build()\n",
    "    tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                               estimatorParamMaps=grid,\n",
    "                               evaluator=evaluator,\n",
    "                               trainRatio=0.8)\n",
    "    models = tvs.fit(train_df)\n",
    "    return models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407d6929-3f53-4921-8cba-e15f802473ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_mf():\n",
    "    mlflow.start_run()\n",
    "    mlflow.set_tracking_uri(\"https://mlflow.lab.karpov.courses\")\n",
    "    mlflow.set_experiment(experiment_name = \"e-lavrushkin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6848804a-3ccf-40a2-a812-4b2e4ff5486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_mf():\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26f795f-ec36-4c04-a8a2-714b39a44b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stage_mf(model):\n",
    "    for i in range(0, len(model.stages)):\n",
    "        stage = model.stages[i]\n",
    "        mlflow.log_param(f'stage_{i}_type', stage)\n",
    "        if type(stage) is VectorAssembler:\n",
    "            mlflow.log_param(f'stage_{i}_input', stage.getInputCols())\n",
    "            mlflow.log_param(f'stage_{i}_output', stage.getOutputCol())\n",
    "        elif type(stage) is StringIndexerModel:\n",
    "            mlflow.log_param(f'stage_{i}_input', stage.getInputCol())\n",
    "            mlflow.log_param(f'stage_{i}_output', stage.getOutputCol())\n",
    "        else:\n",
    "            mlflow.log_param(f'stage_{i}_features', stage.getFeaturesCol())\n",
    "            mlflow.log_param(f'stage_{i}_label', stage.getLabelCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08227e89-b009-42a4-b38d-4df3e218b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_save_model_mf(model):\n",
    "    mv = mlflow.spark.log_model(model,\n",
    "                           artifact_path = \"test\",\n",
    "                           registered_model_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60a1cdf-eb4e-47fc-9733-0a728cbc1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(spark, train_path, test_path):\n",
    "    \"\"\"\n",
    "    Основной процесс задачи.\n",
    "\n",
    "    :param spark: SparkSession\n",
    "    :param train_path: путь до тренировочного датасета\n",
    "    :param test_path: путь до тренировочного датасета\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\", metricName='f1')\n",
    "    train_df = spark.read.parquet(train_path)\n",
    "    test_df = spark.read.parquet(test_path)\n",
    "\n",
    "    gbt = GBTClassifier(labelCol=LABEL_COL)\n",
    "    pipeline = build_pipeline(gbt)\n",
    "\n",
    "    model = optimization(pipeline, gbt, train_df, evaluator)\n",
    "    predict = model.transform(test_df)\n",
    "    \n",
    "    start_mf()\n",
    "    log_stage_mf(model)\n",
    "    \n",
    "    evaluate_model(evaluator, predict, ['f1', 'weightedPrecision', 'weightedRecall', 'accuracy'])\n",
    "    log_save_model_mf(model)\n",
    "    finish_mf()\n",
    "    print('Best model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c15449d-ddf4-40c6-a827-4b2aca99d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_path, test_path):\n",
    "    spark = _spark_session()\n",
    "    process(spark, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfe6c4a-6880-41d9-abd3-df5388937ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spark_session():\n",
    "    \"\"\"\n",
    "    Создание SparkSession.\n",
    "\n",
    "    :return: SparkSession\n",
    "    \"\"\"\n",
    "    return SparkSession.builder.appName('PySparkMLJob').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2514bce-da89-4d71-a5be-04246e9a2186",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Run with UUID aaaeed11b813499aa1a24b5487029ca1 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1/train.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1/test.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_path, test_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(train_path, test_path):\n\u001b[1;32m      2\u001b[0m     spark \u001b[38;5;241m=\u001b[39m _spark_session()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(spark, train_path, test_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m optimization(pipeline, gbt, train_df, evaluator)\n\u001b[1;32m     17\u001b[0m predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_df)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mstart_mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m log_stage_mf(madel)\n\u001b[1;32m     22\u001b[0m evaluate_model(evaluator, predict, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweightedPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweightedRecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mstart_mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_mf\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://mlflow.lab.karpov.courses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me-lavrushkin!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mlflow/tracking/fluent.py:187\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, tags)\u001b[0m\n\u001b[1;32m    185\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(experiment_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m experiment_id\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_active_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nested:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         (\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun with UUID \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is already active. To start a new run, first end the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun, call start_run with nested=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(_active_run_stack[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_id:\n",
      "\u001b[0;31mException\u001b[0m: Run with UUID aaaeed11b813499aa1a24b5487029ca1 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default='train.parquet', help='Please set train datasets path.')\n",
    "    parser.add_argument('--test', type=str, default='test.parquet', help='Please set test datasets path.')\n",
    "    args = parser.parse_args()\n",
    "    train = args.train\n",
    "    test = args.test\n",
    "    main(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a7c9e-e03d-4924-9b5b-7bdd3b405182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe79c8c-0d0f-4ac0-af8d-32903d905c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f55d5-ed87-4e6c-a56f-47ce1e2e5a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc99c7-ca98-4c13-8666-da9403841f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e5880-512d-4f8d-b5a7-615c23eb95fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
