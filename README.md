# Описание
Курс был пройден для на глубокое изучение процессов сбора, хранения, обработки и интеграции данных с использованием широкого круга технологий. Программа охватывает полный цикл работы с данными: от проектирования архитектуры хранилищ до сложных ETL/ELT процессов и применения методов машинного обучения.
## Проектирование DWH
Data Warehouse — централизованное хранилище данных из разных источников. Создание архитектуры Data Warehouse с использованием современных подходов к моделированию данных, таких как Dimensional Modeling, Data Vault, Anchor Modeling и Data Mesh, на основе MPP-СУБД Greenplum.
## Автоматизация ETL-процессов
Разработка ETL-пайплайн для обогащения базы данных, включая создание собственных плагинов для парсинга данных и их последующей загрузки и обработки. В качестве оркестратора использовн Apache Airflow.
## Big Data
Разработка MapReduce-приложение на платформе Hadoop для обработки данных из Object Storage и формирования ежемесячных отчетов.
Разработка системы аналитической отчетности на основе технологии Spark, включающую построение отчетов, выявление корреляций и использование данных для визуализации для дашбордов.
Применение потоковой обработки данных с использованием Apache Kafka и Spark Streaming,
## Облачное хранилище
Развёртывание в облаке виртуальные машины и сервисы, включая JupyterHub, PostgreSQL, MLflow и Spark, с использованием Kubernetes и Docker для управления контейнерами и масштабируемости инфраструктуры.
## Визуализация данных
Разработка интерактивных дашбордов в Tableau, Datalens и Superset для визуализации данных и аналитики.
## Big ML
Разработка системы кредитного скоринга и модели выявления ботов в пользовательских сессиях, используя SparkML и SparkUDF
## Управление моделями
Реализация версионирование данных с DVC и настройка трекинга моделей в MLflow, обеспечив контроль над изменениями и воспроизводимость экспериментов.
## Управление данными
Проведение тестирование качества датасета с использованием Deequ, обеспечив автоматическую проверку данных и выявление аномалий.


## Как я могу улучшить работу с данными:
1.	Оптимизация хранения и обработки данных – проектирование и внедрение централизованного Data Warehouse на MPP-СУБД Greenplum, что улучшает доступность и скорость аналитики данных из разных источников.
2.	Автоматизация ETL-процессов – разработка и оркестрация ETL-пайплайнов в Apache Airflow, что снижает ручной труд, ускоряет обработку данных и обеспечивает стабильность обновления базы.
3.	Работа с Big Data – разработка MapReduce-приложений и аналитики на Spark, что позволяет эффективно обрабатывать большие объемы данных и выявлять скрытые закономерности для бизнеса.
4.	Разработка моделей машинного обучения – создание кредитного скоринга и детектирования ботов на SparkML с управлением версиями моделей в MLflow, что повышает точность предсказаний и прозрачность моделей.
5.	Визуализация и аналитика данных – создание интерактивных дашбордов в Tableau, Superset и Datalens, что помогает принимать обоснованные решения на основе данных и упрощает мониторинг ключевых показателей.
